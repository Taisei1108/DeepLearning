シンプルなresnet50を使って(F値0.50)、CRFの単一項の確率を0.7→0.9にするとどうなるのかを確かめる
{'dataset_root': '../../datasets/Columbia/', 'train_list': 'data_manage/train.txt', 'val_list': 'data_manage/val.txt', 'test_list': 'data_manage/test.txt', 'cam_batch_size': 16, 'cam_network': 'resnet50_cam', 'cam_crop_size': 256, 'cam_output_class': 2, 'cam_learning_rate': 1e-05, 'cam_momentum': 0.99, 'cam_num_epochs': 150, 'cam_affine_degree': 10, 'cam_scale': (1.0, 1.5), 'cam_weights_name': 'sess/res50_cam.pth', 'cam_out_dir': 'result/cam/', 'segmentation_out_dir_CAM': 'result/seg/CAM/', 'segmentation_out_dir_CRF': 'result/seg/CRF/', 'train_cam_pass': True, 'eval_cam_pass': True, 'make_cam_pass': True, 'eval_seg_pass': True}
dataset_root ../../datasets/Columbia/
train_list data_manage/train.txt
val_list data_manage/val.txt
test_list data_manage/test.txt
cam_batch_size 16
cam_network resnet50_cam
cam_crop_size 256
cam_output_class 2
cam_learning_rate 1e-05
cam_momentum 0.99
cam_num_epochs 150
cam_affine_degree 10
cam_scale (1.0, 1.5)
cam_weights_name sess/res50_cam.pth
cam_out_dir result/cam/
segmentation_out_dir_CAM result/seg/CAM/
segmentation_out_dir_CRF result/seg/CRF/
train_cam_pass True
eval_cam_pass True
make_cam_pass True
eval_seg_pass True
data_manage/train.txt
290
290
data_manage/val.txt
37
37
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Epoch 1/150
-------------
val Loss: 0.7152 Acc: 0.5676
Epoch 2/150
-------------
train Loss: 0.7130 Acc: 0.4828
val Loss: 0.7180 Acc: 0.4324
Epoch 3/150
-------------
train Loss: 0.6918 Acc: 0.5310
val Loss: 0.7035 Acc: 0.5135
Epoch 4/150
-------------
train Loss: 0.6797 Acc: 0.5379
val Loss: 0.7080 Acc: 0.5405
Epoch 5/150
-------------
train Loss: 0.6672 Acc: 0.5690
val Loss: 0.6782 Acc: 0.6216
Epoch 6/150
-------------
train Loss: 0.6462 Acc: 0.6552
val Loss: 0.6452 Acc: 0.6757
Epoch 7/150
-------------
train Loss: 0.6221 Acc: 0.7241
val Loss: 0.6222 Acc: 0.7027
Epoch 8/150
-------------
train Loss: 0.6034 Acc: 0.7345
val Loss: 0.6012 Acc: 0.7027
Epoch 9/150
-------------
train Loss: 0.5736 Acc: 0.7759
val Loss: 0.5760 Acc: 0.7027
Epoch 10/150
-------------
train Loss: 0.5497 Acc: 0.7793
val Loss: 0.5561 Acc: 0.6757
Epoch 11/150
-------------
train Loss: 0.5373 Acc: 0.7517
val Loss: 0.5450 Acc: 0.7297
Epoch 12/150
-------------
train Loss: 0.5228 Acc: 0.7690
val Loss: 0.5077 Acc: 0.8108
Epoch 13/150
-------------
train Loss: 0.5023 Acc: 0.8138
val Loss: 0.5004 Acc: 0.8378
Epoch 14/150
-------------
train Loss: 0.4958 Acc: 0.8034
val Loss: 0.4919 Acc: 0.8378
Epoch 15/150
-------------
train Loss: 0.4888 Acc: 0.8069
val Loss: 0.4722 Acc: 0.8108
Epoch 16/150
-------------
train Loss: 0.4497 Acc: 0.8552
val Loss: 0.4263 Acc: 0.8649
Epoch 17/150
-------------
train Loss: 0.4660 Acc: 0.8207
val Loss: 0.4334 Acc: 0.8649
Epoch 18/150
-------------
train Loss: 0.4196 Acc: 0.8759
val Loss: 0.4230 Acc: 0.8378
Epoch 19/150
-------------
train Loss: 0.4481 Acc: 0.8172
val Loss: 0.4072 Acc: 0.8919
Epoch 20/150
-------------
train Loss: 0.4241 Acc: 0.8276
val Loss: 0.3873 Acc: 0.8649
Epoch 21/150
-------------
train Loss: 0.4240 Acc: 0.8310
val Loss: 0.3728 Acc: 0.8649
Epoch 22/150
-------------
train Loss: 0.3767 Acc: 0.8483
val Loss: 0.3650 Acc: 0.8378
Epoch 23/150
-------------
train Loss: 0.3952 Acc: 0.8517
val Loss: 0.3646 Acc: 0.8919
Epoch 24/150
-------------
train Loss: 0.4224 Acc: 0.8241
val Loss: 0.3530 Acc: 0.8919
Epoch 25/150
-------------
train Loss: 0.3909 Acc: 0.8310
val Loss: 0.3499 Acc: 0.8649
Epoch 26/150
-------------
train Loss: 0.3836 Acc: 0.8414
val Loss: 0.3323 Acc: 0.8919
Epoch 27/150
-------------
train Loss: 0.3505 Acc: 0.8759
val Loss: 0.3281 Acc: 0.8919
Epoch 28/150
-------------
train Loss: 0.3233 Acc: 0.8586
val Loss: 0.3308 Acc: 0.8649
Epoch 29/150
-------------
train Loss: 0.3629 Acc: 0.8379
val Loss: 0.3255 Acc: 0.8919
Epoch 30/150
-------------
train Loss: 0.3352 Acc: 0.8793
val Loss: 0.3276 Acc: 0.8919
Epoch 31/150
-------------
train Loss: 0.3228 Acc: 0.8931
val Loss: 0.3126 Acc: 0.8919
Epoch 32/150
-------------
train Loss: 0.3405 Acc: 0.8690
val Loss: 0.3222 Acc: 0.8919
Epoch 33/150
-------------
train Loss: 0.3289 Acc: 0.8828
val Loss: 0.2866 Acc: 0.9189
Epoch 34/150
-------------
train Loss: 0.3282 Acc: 0.8759
val Loss: 0.2962 Acc: 0.8649
Epoch 35/150
-------------
train Loss: 0.3545 Acc: 0.8172
val Loss: 0.2865 Acc: 0.9189
Epoch 36/150
-------------
train Loss: 0.3172 Acc: 0.8828
val Loss: 0.2767 Acc: 0.9189
Epoch 37/150
-------------
train Loss: 0.3242 Acc: 0.8690
val Loss: 0.2664 Acc: 0.9189
Epoch 38/150
-------------
train Loss: 0.3237 Acc: 0.8621
val Loss: 0.2740 Acc: 0.9189
Epoch 39/150
-------------
train Loss: 0.3051 Acc: 0.8759
val Loss: 0.2548 Acc: 0.9189
Epoch 40/150
-------------
train Loss: 0.3086 Acc: 0.8931
val Loss: 0.2600 Acc: 0.9189
Epoch 41/150
-------------
train Loss: 0.3381 Acc: 0.8586
val Loss: 0.2525 Acc: 0.9189
Epoch 42/150
-------------
train Loss: 0.3333 Acc: 0.8655
val Loss: 0.2447 Acc: 0.9459
Epoch 43/150
-------------
train Loss: 0.2880 Acc: 0.9034
val Loss: 0.2401 Acc: 0.9459
Epoch 44/150
-------------
train Loss: 0.3007 Acc: 0.8759
val Loss: 0.2486 Acc: 0.9189
Epoch 45/150
-------------
train Loss: 0.3067 Acc: 0.8931
val Loss: 0.2768 Acc: 0.9189
Epoch 46/150
-------------
train Loss: 0.2850 Acc: 0.8759
val Loss: 0.2800 Acc: 0.9189
Epoch 47/150
-------------
train Loss: 0.3011 Acc: 0.8690
val Loss: 0.2675 Acc: 0.9189
Epoch 48/150
-------------
train Loss: 0.2538 Acc: 0.9172
val Loss: 0.2658 Acc: 0.8919
Epoch 49/150
-------------
train Loss: 0.2946 Acc: 0.8690
val Loss: 0.2565 Acc: 0.9189
Epoch 50/150
-------------
train Loss: 0.2711 Acc: 0.8966
val Loss: 0.2393 Acc: 0.9189
Epoch 51/150
-------------
train Loss: 0.2576 Acc: 0.9207
val Loss: 0.2346 Acc: 0.9189
Epoch 52/150
-------------
train Loss: 0.2351 Acc: 0.9207
val Loss: 0.2651 Acc: 0.8919
Epoch 53/150
-------------
train Loss: 0.2489 Acc: 0.9207
val Loss: 0.2665 Acc: 0.9189
Epoch 54/150
-------------
train Loss: 0.2351 Acc: 0.9310
val Loss: 0.2249 Acc: 0.9189
Epoch 55/150
-------------
train Loss: 0.2534 Acc: 0.9138
val Loss: 0.2122 Acc: 0.9189
Epoch 56/150
-------------
train Loss: 0.2780 Acc: 0.9000
val Loss: 0.2843 Acc: 0.8919
Epoch 57/150
-------------
train Loss: 0.2574 Acc: 0.8931
val Loss: 0.2340 Acc: 0.9189
Epoch 58/150
-------------
train Loss: 0.2772 Acc: 0.9000
val Loss: 0.2386 Acc: 0.9189
Epoch 59/150
-------------
train Loss: 0.2287 Acc: 0.9345
val Loss: 0.2194 Acc: 0.9189
Epoch 60/150
-------------
train Loss: 0.2434 Acc: 0.9138
val Loss: 0.2459 Acc: 0.9189
Epoch 61/150
-------------
train Loss: 0.2139 Acc: 0.9345
val Loss: 0.2364 Acc: 0.9189
Epoch 62/150
-------------
train Loss: 0.2403 Acc: 0.9172
val Loss: 0.2258 Acc: 0.9189
Epoch 63/150
-------------
train Loss: 0.2413 Acc: 0.9103
val Loss: 0.2722 Acc: 0.8919
Epoch 64/150
-------------
train Loss: 0.2281 Acc: 0.9207
val Loss: 0.2444 Acc: 0.8919
Epoch 65/150
-------------
train Loss: 0.2295 Acc: 0.9069
val Loss: 0.2087 Acc: 0.9189
Epoch 66/150
-------------
train Loss: 0.2255 Acc: 0.9138
val Loss: 0.2257 Acc: 0.8919
Epoch 67/150
-------------
train Loss: 0.2633 Acc: 0.8931
val Loss: 0.2055 Acc: 0.9189
Epoch 68/150
-------------
train Loss: 0.2608 Acc: 0.8862
val Loss: 0.2688 Acc: 0.8919
Epoch 69/150
-------------
train Loss: 0.2276 Acc: 0.9103
val Loss: 0.2523 Acc: 0.9189
Epoch 70/150
-------------
train Loss: 0.2053 Acc: 0.9276
val Loss: 0.2530 Acc: 0.9189
Epoch 71/150
-------------
train Loss: 0.2089 Acc: 0.9379
val Loss: 0.2308 Acc: 0.9189
Epoch 72/150
-------------
train Loss: 0.2316 Acc: 0.9241
val Loss: 0.2365 Acc: 0.9189
Epoch 73/150
-------------
train Loss: 0.2271 Acc: 0.9207
val Loss: 0.2039 Acc: 0.9189
Epoch 74/150
-------------
train Loss: 0.2396 Acc: 0.9000
val Loss: 0.2159 Acc: 0.9189
Epoch 75/150
-------------
train Loss: 0.2184 Acc: 0.9207
val Loss: 0.2260 Acc: 0.9189
Epoch 76/150
-------------
train Loss: 0.2380 Acc: 0.9034
val Loss: 0.2029 Acc: 0.9189
Epoch 77/150
-------------
train Loss: 0.2592 Acc: 0.8966
val Loss: 0.1979 Acc: 0.8919
Epoch 78/150
-------------
train Loss: 0.2389 Acc: 0.9241
val Loss: 0.2066 Acc: 0.9189
Epoch 79/150
-------------
train Loss: 0.2366 Acc: 0.9172
val Loss: 0.2325 Acc: 0.9189
Epoch 80/150
-------------
train Loss: 0.2541 Acc: 0.8897
val Loss: 0.2906 Acc: 0.8919
Epoch 81/150
-------------
train Loss: 0.2048 Acc: 0.9241
val Loss: 0.2629 Acc: 0.8919
Epoch 82/150
-------------
train Loss: 0.1982 Acc: 0.9276
val Loss: 0.2474 Acc: 0.9189
Epoch 83/150
-------------
train Loss: 0.2397 Acc: 0.9034
val Loss: 0.2339 Acc: 0.9189
Epoch 84/150
-------------
train Loss: 0.2311 Acc: 0.8966
val Loss: 0.2276 Acc: 0.9189
Epoch 85/150
-------------
train Loss: 0.2014 Acc: 0.9241
val Loss: 0.2322 Acc: 0.9189
Epoch 86/150
-------------
train Loss: 0.2024 Acc: 0.9276
val Loss: 0.2426 Acc: 0.9189
Epoch 87/150
-------------
train Loss: 0.1618 Acc: 0.9552
val Loss: 0.2137 Acc: 0.9189
Epoch 88/150
-------------
train Loss: 0.2181 Acc: 0.9207
val Loss: 0.1879 Acc: 0.9459
Epoch 89/150
-------------
train Loss: 0.2088 Acc: 0.9276
val Loss: 0.2064 Acc: 0.9189
Epoch 90/150
-------------
train Loss: 0.2148 Acc: 0.9034
val Loss: 0.2560 Acc: 0.9189
Epoch 91/150
-------------
train Loss: 0.1955 Acc: 0.9241
val Loss: 0.2591 Acc: 0.9189
Epoch 92/150
-------------
train Loss: 0.1834 Acc: 0.9310
val Loss: 0.2276 Acc: 0.9189
Epoch 93/150
-------------
train Loss: 0.2119 Acc: 0.9241
val Loss: 0.2355 Acc: 0.8919
Epoch 94/150
-------------
train Loss: 0.2500 Acc: 0.8931
val Loss: 0.2475 Acc: 0.9189
Epoch 95/150
-------------
train Loss: 0.1737 Acc: 0.9483
val Loss: 0.2176 Acc: 0.9189
Epoch 96/150
-------------
train Loss: 0.1986 Acc: 0.9138
val Loss: 0.2197 Acc: 0.8919
Epoch 97/150
-------------
train Loss: 0.2496 Acc: 0.9172
val Loss: 0.2223 Acc: 0.9189
Epoch 98/150
-------------
train Loss: 0.1767 Acc: 0.9379
val Loss: 0.2489 Acc: 0.8919
Epoch 99/150
-------------
train Loss: 0.1650 Acc: 0.9483
val Loss: 0.1999 Acc: 0.9189
Epoch 100/150
-------------
train Loss: 0.2378 Acc: 0.9138
val Loss: 0.2210 Acc: 0.9459
Epoch 101/150
-------------
train Loss: 0.1963 Acc: 0.9276
val Loss: 0.2124 Acc: 0.9459
Epoch 102/150
-------------
train Loss: 0.1723 Acc: 0.9448
val Loss: 0.1742 Acc: 0.9189
Epoch 103/150
-------------
train Loss: 0.1917 Acc: 0.9448
val Loss: 0.2133 Acc: 0.9189
Epoch 104/150
-------------
train Loss: 0.2281 Acc: 0.9138
val Loss: 0.2180 Acc: 0.9189
Epoch 105/150
-------------
train Loss: 0.1621 Acc: 0.9483
val Loss: 0.2760 Acc: 0.8919
Epoch 106/150
-------------
train Loss: 0.2085 Acc: 0.9241
val Loss: 0.1680 Acc: 0.9459
Epoch 107/150
-------------
train Loss: 0.1817 Acc: 0.9448
val Loss: 0.1899 Acc: 0.8919
Epoch 108/150
-------------
train Loss: 0.1488 Acc: 0.9621
val Loss: 0.2301 Acc: 0.9189
Epoch 109/150
-------------
train Loss: 0.1667 Acc: 0.9552
val Loss: 0.2733 Acc: 0.8919
Epoch 110/150
-------------
train Loss: 0.1776 Acc: 0.9207
val Loss: 0.2663 Acc: 0.8919
Epoch 111/150
-------------
train Loss: 0.1956 Acc: 0.9103
val Loss: 0.2577 Acc: 0.8919
Epoch 112/150
-------------
train Loss: 0.1791 Acc: 0.9345
val Loss: 0.2063 Acc: 0.9459
Epoch 113/150
-------------
train Loss: 0.1779 Acc: 0.9310
val Loss: 0.1908 Acc: 0.9459
Epoch 114/150
-------------
train Loss: 0.1971 Acc: 0.9138
val Loss: 0.2349 Acc: 0.8919
Epoch 115/150
-------------
train Loss: 0.2124 Acc: 0.9138
val Loss: 0.2125 Acc: 0.9189
Epoch 116/150
-------------
train Loss: 0.1486 Acc: 0.9586
val Loss: 0.1777 Acc: 0.9189
Epoch 117/150
-------------
train Loss: 0.1506 Acc: 0.9483
val Loss: 0.1972 Acc: 0.9459
Epoch 118/150
-------------
train Loss: 0.1891 Acc: 0.9448
val Loss: 0.2512 Acc: 0.9189
Epoch 119/150
-------------
train Loss: 0.1606 Acc: 0.9483
val Loss: 0.2361 Acc: 0.8919
Epoch 120/150
-------------
train Loss: 0.1613 Acc: 0.9483
val Loss: 0.2260 Acc: 0.9189
Epoch 121/150
-------------
train Loss: 0.1781 Acc: 0.9276
val Loss: 0.2147 Acc: 0.9189
Epoch 122/150
-------------
train Loss: 0.1555 Acc: 0.9517
val Loss: 0.2324 Acc: 0.8919
Epoch 123/150
-------------
train Loss: 0.1738 Acc: 0.9379
val Loss: 0.2926 Acc: 0.8919
Epoch 124/150
-------------
train Loss: 0.1697 Acc: 0.9379
val Loss: 0.2549 Acc: 0.8919
Epoch 125/150
-------------
train Loss: 0.1402 Acc: 0.9655
val Loss: 0.1780 Acc: 0.9459
Epoch 126/150
-------------
train Loss: 0.1663 Acc: 0.9414
val Loss: 0.2371 Acc: 0.9189
Epoch 127/150
-------------
train Loss: 0.1620 Acc: 0.9448
val Loss: 0.2090 Acc: 0.9459
Epoch 128/150
-------------
train Loss: 0.1792 Acc: 0.9552
val Loss: 0.2185 Acc: 0.9459
Epoch 129/150
-------------
train Loss: 0.1535 Acc: 0.9414
val Loss: 0.1870 Acc: 0.9459
Epoch 130/150
-------------
train Loss: 0.1680 Acc: 0.9345
val Loss: 0.1904 Acc: 0.9459
Epoch 131/150
-------------
train Loss: 0.1873 Acc: 0.9448
val Loss: 0.2335 Acc: 0.9189
Epoch 132/150
-------------
train Loss: 0.1632 Acc: 0.9414
val Loss: 0.1876 Acc: 0.9459
Epoch 133/150
-------------
train Loss: 0.1489 Acc: 0.9517
val Loss: 0.1858 Acc: 0.9459
Epoch 134/150
-------------
train Loss: 0.1555 Acc: 0.9448
val Loss: 0.2480 Acc: 0.9189
Epoch 135/150
-------------
train Loss: 0.1298 Acc: 0.9586
val Loss: 0.2759 Acc: 0.8919
Epoch 136/150
-------------
train Loss: 0.1404 Acc: 0.9552
val Loss: 0.1964 Acc: 0.9459
Epoch 137/150
-------------
train Loss: 0.1404 Acc: 0.9586
val Loss: 0.2245 Acc: 0.9189
Epoch 138/150
-------------
train Loss: 0.1861 Acc: 0.9276
val Loss: 0.2605 Acc: 0.8919
Epoch 139/150
-------------
train Loss: 0.1868 Acc: 0.9414
val Loss: 0.2489 Acc: 0.8919
Epoch 140/150
-------------
train Loss: 0.1420 Acc: 0.9517
val Loss: 0.2930 Acc: 0.9189
Epoch 141/150
-------------
train Loss: 0.2302 Acc: 0.9172
val Loss: 0.1964 Acc: 0.9459
Epoch 142/150
-------------
train Loss: 0.1517 Acc: 0.9345
val Loss: 0.2560 Acc: 0.8919
Epoch 143/150
-------------
train Loss: 0.1672 Acc: 0.9276
val Loss: 0.2881 Acc: 0.8919
Epoch 144/150
-------------
train Loss: 0.2234 Acc: 0.9172
val Loss: 0.2455 Acc: 0.8919
Epoch 145/150
-------------
train Loss: 0.1543 Acc: 0.9448
val Loss: 0.2525 Acc: 0.9189
Epoch 146/150
-------------
train Loss: 0.1312 Acc: 0.9621
val Loss: 0.2484 Acc: 0.9189
Epoch 147/150
-------------
train Loss: 0.1404 Acc: 0.9517
val Loss: 0.2956 Acc: 0.8919
Epoch 148/150
-------------
train Loss: 0.1243 Acc: 0.9517
val Loss: 0.2722 Acc: 0.8919
Epoch 149/150
-------------
train Loss: 0.1396 Acc: 0.9621
val Loss: 0.2226 Acc: 0.9189
Epoch 150/150
-------------
train Loss: 0.1662 Acc: 0.9379
val Loss: 0.2111 Acc: 0.9189
max_val= 0.945945945945946
data_manage/test.txt
36
36
test Loss: 0.3648 Acc: 0.8333
data_manage/test.txt
36
36
CRF Done!
canong3_05_sub_04 : TN ( 0 , 0 )
CRF Done!
nikond70_02_sub_04 : FP ( 1 , 0 )
CRF Done!
canong3_02_sub_04 : TN ( 0 , 0 )
CRF Done!
canonxt_kodakdcs330_sub_29 : TP ( 1 , 1 )
CRF Done!
canong3_nikond70_sub_19 : TP ( 1 , 1 )
CRF Done!
nikond70_08_sub_09 : TN ( 0 , 0 )
CRF Done!
canong3_canonxt_sub_30 : TP ( 1 , 1 )
CRF Done!
canong3_08_sub_02 : TN ( 0 , 0 )
CRF Done!
nikond70_kodakdcs330_sub_19 : TP ( 1 , 1 )
CRF Done!
canonxt_20_sub_08 : TN ( 0 , 0 )
CRF Done!
nikond70_canonxt_sub_18 : FN ( 0 , 1 )
CRF Done!
nikond70_canonxt_sub_20 : FN ( 0 , 1 )
CRF Done!
nikond70_canonxt_sub_15 : FN ( 0 , 1 )
CRF Done!
canonxt_38_sub_01 : TN ( 0 , 0 )
CRF Done!
canong3_kodakdcs330_sub_20 : TP ( 1 , 1 )
CRF Done!
canonxt_kodakdcs330_sub_26 : TP ( 1 , 1 )
CRF Done!
canonxt_02_sub_07 : TN ( 0 , 0 )
CRF Done!
canonxt_11_sub_05 : TN ( 0 , 0 )
CRF Done!
canong3_canonxt_sub_04 : TP ( 1 , 1 )
CRF Done!
canong3_05_sub_07 : TN ( 0 , 0 )
CRF Done!
canong3_kodakdcs330_sub_21 : TP ( 1 , 1 )
CRF Done!
nikond70_kodakdcs330_sub_07 : TP ( 1 , 1 )
CRF Done!
canonxt_kodakdcs330_sub_20 : TP ( 1 , 1 )
CRF Done!
canonxt_11_sub_08 : FP ( 1 , 0 )
CRF Done!
nikond70_kodakdcs330_sub_14 : TP ( 1 , 1 )
CRF Done!
canonxt_20_sub_06 : TN ( 0 , 0 )
CRF Done!
canong3_nikond70_sub_13 : TP ( 1 , 1 )
CRF Done!
canong3_kodakdcs330_sub_24 : TP ( 1 , 1 )
CRF Done!
canong3_05_sub_02 : TN ( 0 , 0 )
CRF Done!
canong3_canonxt_sub_22 : TP ( 1 , 1 )
CRF Done!
canonxt_08_sub_04 : TN ( 0 , 0 )
CRF Done!
canonxt_02_sub_08 : TN ( 0 , 0 )
CRF Done!
nikond70_canonxt_sub_24 : FN ( 0 , 1 )
CRF Done!
canonxt_26_sub_03 : TN ( 0 , 0 )
CRF Done!
canonxt_23_sub_05 : TN ( 0 , 0 )
CRF Done!
canonxt_02_sub_05 : TN ( 0 , 0 )
test_acc= tensor(0.8333, device='cuda:0', dtype=torch.float64)
36
canonxt_kodakdcs330_sub_29 :iou: 0.49432262504301044 F_measure: 0.6616009377878255
canong3_canonxt_sub_22 :iou: 0.18608125492799935 F_measure: 0.31377488541338655
canonxt_kodakdcs330_sub_26 :iou: 0.502098232756073 F_measure: 0.6685291571574722
canong3_kodakdcs330_sub_24 :iou: 0.703065056737875 F_measure: 0.8256467408057288
canonxt_kodakdcs330_sub_20 :iou: 0.7450813630436015 F_measure: 0.8539216323347846
canong3_canonxt_sub_30 :iou: 0.06858415575438731 F_measure: 0.1283645380385956
canong3_nikond70_sub_19 :iou: 0.12200021143884132 F_measure: 0.21746914161876946
canong3_canonxt_sub_04 :iou: 0.2148273484990913 F_measure: 0.35367552231106525
nikond70_kodakdcs330_sub_14 :iou: 0.3166627082414413 F_measure: 0.4810080915322372
canong3_nikond70_sub_13 :iou: 0.37306431043905997 F_measure: 0.543403987129731
nikond70_kodakdcs330_sub_07 :iou: 0.5863133328314387 F_measure: 0.7392150348820654
canong3_kodakdcs330_sub_21 :iou: 0.517697767574785 F_measure: 0.6822145734615443
canong3_kodakdcs330_sub_20 :iou: 0.3761343292795852 F_measure: 0.5466535079849273
nikond70_kodakdcs330_sub_19 :iou: 0.2580478211365283 F_measure: 0.4102353134770446
Result IoU: 5.4639805177037175 / 14 = 0.3902843226931227
Result F_measure: 7.425713063935178 / 14 = 0.5304080759953699
36
canong3_kodakdcs330_sub_24 :iou: 0.8380126552613077 F_measure: 0.9118682103330444
canong3_nikond70_sub_13 :iou: 0.5660106147722247 F_measure: 0.7228694485631576
canong3_kodakdcs330_sub_21 :iou: 0.5536628945801072 F_measure: 0.7127194663804339
canong3_kodakdcs330_sub_20 :iou: 0.450441609421001 F_measure: 0.6211096075778079
nikond70_kodakdcs330_sub_19 :iou: 0.24212462635088527 F_measure: 0.38985560903369126
canong3_nikond70_sub_19 :iou: 0.060867495686321174 F_measure: 0.11475042063937184
canong3_canonxt_sub_04 :iou: 0.050616050616050616 F_measure: 0.09635499207606975
nikond70_kodakdcs330_sub_07 :iou: 0.7592899639130011 F_measure: 0.8631777358908971
canong3_canonxt_sub_22 :iou: 0.14575184702303345 F_measure: 0.25442131714949506
canonxt_kodakdcs330_sub_20 :iou: 0.8553606942686676 F_measure: 0.9220424868446696
canong3_canonxt_sub_30 :iou: 0 F_measure: 0
nikond70_kodakdcs330_sub_14 :iou: 0.4675386706837638 F_measure: 0.6371739021581293
canonxt_kodakdcs330_sub_26 :iou: 0.5282667411753749 F_measure: 0.6913279297946249
canonxt_kodakdcs330_sub_29 :iou: 0.6032515474681184 F_measure: 0.7525351195460042
Result IoU: 6.121195411219857 / 14 = 0.4372282436585612
Result F_measure: 7.690206245987396 / 14 = 0.5493004461419568
