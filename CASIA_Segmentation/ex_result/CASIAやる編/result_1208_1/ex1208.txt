CASIAでやるぞ編、Columbiaとの変更箇所が多くて大変
やってみたらそもそも２値分類ができてなくて、F値も0.1とかなのでやりなおす
学習の際のフリップを抜いたりしたら、train_lossがまあまあ下がって行けたが
過学習気味だった。SAのCRFも0.17までになったが低いので
データセットを絞って学習してみる。
{'dataset_root': '../../datasets/CASIAv2_data/', 'train_list': 'data_manage_CASIA/train_CASIA.txt', 'val_list': 'data_manage_CASIA/val_CASIA.txt', 'test_list': 'data_manage_CASIA/test_CASIA.txt', 'cam_batch_size': 16, 'cam_network': 'resnet50_cam', 'cam_crop_size': 256, 'cam_output_class': 2, 'cam_learning_rate': 1e-05, 'cam_momentum': 0.99, 'cam_num_epochs': 150, 'cam_affine_degree': 5, 'cam_scale': (1.0, 1.5), 'cam_weights_name': 'sess/res50_cam.pth', 'cam_out_dir': 'result/cam/', 'segmentation_out_dir_CAM': 'result/seg/CAM/', 'segmentation_out_dir_CRF': 'result/seg/CRF/', 'segmentation_out_dir_SA': 'result/seg/SA/', 'segmentation_out_dir_SA_CRF': 'result/seg/SA_CRF/', 'train_cam_pass': True, 'eval_cam_pass': True, 'make_cam_pass': True, 'eval_seg_pass': True}
dataset_root ../../datasets/CASIAv2_data/
train_list data_manage_CASIA/train_CASIA.txt
val_list data_manage_CASIA/val_CASIA.txt
test_list data_manage_CASIA/test_CASIA.txt
cam_batch_size 16
cam_network resnet50_cam
cam_crop_size 256
cam_output_class 2
cam_learning_rate 1e-05
cam_momentum 0.99
cam_num_epochs 150
cam_affine_degree 5
cam_scale (1.0, 1.5)
cam_weights_name sess/res50_cam.pth
cam_out_dir result/cam/
segmentation_out_dir_CAM result/seg/CAM/
segmentation_out_dir_CRF result/seg/CRF/
segmentation_out_dir_SA result/seg/SA/
segmentation_out_dir_SA_CRF result/seg/SA_CRF/
train_cam_pass True
eval_cam_pass True
make_cam_pass True
eval_seg_pass True
data_manage_CASIA/train_CASIA.txt
10090
10090
data_manage_CASIA/val_CASIA.txt
1263
1263
DataParallel(
  (module): Net(
    (resnet50): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=1000, bias=True)
    )
    (layer1): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (layer2): Sequential(
      (0): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (layer3): Sequential(
      (0): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (layer4): Sequential(
      (0): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (Self_Attn1): Self_Attn(
      (query_conv): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
      (key_conv): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
      (value_conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      (softmax): Softmax(dim=-1)
    )
    (Self_Attn2): Self_Attn(
      (query_conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (key_conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (value_conv): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
      (softmax): Softmax(dim=-1)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Epoch 1/150
-------------
val Loss: 0.7739 Acc: 0.4109
Epoch 2/150
-------------
train Loss: 0.6617 Acc: 0.6064
val Loss: 0.6217 Acc: 0.6603
Epoch 3/150
-------------
train Loss: 0.6097 Acc: 0.6633
val Loss: 0.5958 Acc: 0.6683
Epoch 4/150
-------------
train Loss: 0.5903 Acc: 0.6822
val Loss: 0.5874 Acc: 0.6785
Epoch 5/150
-------------
train Loss: 0.5835 Acc: 0.6924
val Loss: 0.5727 Acc: 0.6928
Epoch 6/150
-------------
train Loss: 0.5735 Acc: 0.7027
val Loss: 0.5716 Acc: 0.6888
Epoch 7/150
-------------
train Loss: 0.5687 Acc: 0.7019
val Loss: 0.5661 Acc: 0.6888
Epoch 8/150
-------------
train Loss: 0.5599 Acc: 0.7088
val Loss: 0.5614 Acc: 0.7015
Epoch 9/150
-------------
train Loss: 0.5586 Acc: 0.7079
val Loss: 0.5627 Acc: 0.7047
Epoch 10/150
-------------
train Loss: 0.5459 Acc: 0.7183
val Loss: 0.5548 Acc: 0.6999
Epoch 11/150
-------------
train Loss: 0.5384 Acc: 0.7265
val Loss: 0.5580 Acc: 0.6952
Epoch 12/150
-------------
train Loss: 0.5375 Acc: 0.7244
val Loss: 0.5543 Acc: 0.6999
Epoch 13/150
-------------
train Loss: 0.5317 Acc: 0.7289
val Loss: 0.5535 Acc: 0.7110
Epoch 14/150
-------------
train Loss: 0.5270 Acc: 0.7321
val Loss: 0.5528 Acc: 0.7031
Epoch 15/150
-------------
train Loss: 0.5211 Acc: 0.7377
val Loss: 0.5498 Acc: 0.6975
Epoch 16/150
-------------
train Loss: 0.5273 Acc: 0.7311
val Loss: 0.5477 Acc: 0.6975
Epoch 17/150
-------------
train Loss: 0.5270 Acc: 0.7324
val Loss: 0.5463 Acc: 0.7039
Epoch 18/150
-------------
train Loss: 0.5154 Acc: 0.7338
val Loss: 0.5461 Acc: 0.7078
Epoch 19/150
-------------
train Loss: 0.5195 Acc: 0.7381
val Loss: 0.5483 Acc: 0.7007
Epoch 20/150
-------------
train Loss: 0.5069 Acc: 0.7462
val Loss: 0.5498 Acc: 0.7078
Epoch 21/150
-------------
train Loss: 0.5026 Acc: 0.7503
val Loss: 0.5439 Acc: 0.7134
Epoch 22/150
-------------
train Loss: 0.5035 Acc: 0.7447
val Loss: 0.5452 Acc: 0.7134
Epoch 23/150
-------------
train Loss: 0.4986 Acc: 0.7510
val Loss: 0.5421 Acc: 0.7150
Epoch 24/150
-------------
train Loss: 0.4911 Acc: 0.7557
val Loss: 0.5429 Acc: 0.7134
Epoch 25/150
-------------
train Loss: 0.5033 Acc: 0.7465
val Loss: 0.5477 Acc: 0.7197
Epoch 26/150
-------------
train Loss: 0.4939 Acc: 0.7519
val Loss: 0.5433 Acc: 0.7181
Epoch 27/150
-------------
train Loss: 0.4837 Acc: 0.7631
val Loss: 0.5575 Acc: 0.7158
Epoch 28/150
-------------
train Loss: 0.4936 Acc: 0.7516
val Loss: 0.5410 Acc: 0.7134
Epoch 29/150
-------------
train Loss: 0.4835 Acc: 0.7642
val Loss: 0.5467 Acc: 0.7165
Epoch 30/150
-------------
train Loss: 0.4804 Acc: 0.7645
val Loss: 0.5438 Acc: 0.7110
Epoch 31/150
-------------
train Loss: 0.4761 Acc: 0.7681
val Loss: 0.5576 Acc: 0.7078
Epoch 32/150
-------------
train Loss: 0.4816 Acc: 0.7582
val Loss: 0.5540 Acc: 0.7063
Epoch 33/150
-------------
train Loss: 0.4765 Acc: 0.7659
val Loss: 0.5466 Acc: 0.7126
Epoch 34/150
-------------
train Loss: 0.4706 Acc: 0.7661
val Loss: 0.5519 Acc: 0.7110
Epoch 35/150
-------------
train Loss: 0.4622 Acc: 0.7738
val Loss: 0.5465 Acc: 0.7173
Epoch 36/150
-------------
train Loss: 0.4721 Acc: 0.7672
val Loss: 0.5618 Acc: 0.7221
Epoch 37/150
-------------
train Loss: 0.4649 Acc: 0.7682
val Loss: 0.5515 Acc: 0.7094
Epoch 38/150
-------------
train Loss: 0.4654 Acc: 0.7708
val Loss: 0.5558 Acc: 0.7173
Epoch 39/150
-------------
train Loss: 0.4624 Acc: 0.7778
val Loss: 0.5579 Acc: 0.7047
Epoch 40/150
-------------
train Loss: 0.4543 Acc: 0.7764
val Loss: 0.5558 Acc: 0.7181
Epoch 41/150
-------------
train Loss: 0.4520 Acc: 0.7780
val Loss: 0.5542 Acc: 0.7189
Epoch 42/150
-------------
train Loss: 0.4550 Acc: 0.7793
val Loss: 0.5536 Acc: 0.7142
Epoch 43/150
-------------
train Loss: 0.4428 Acc: 0.7874
val Loss: 0.5643 Acc: 0.7276
Epoch 44/150
-------------
train Loss: 0.4497 Acc: 0.7854
val Loss: 0.5586 Acc: 0.7142
Epoch 45/150
-------------
train Loss: 0.4510 Acc: 0.7808
val Loss: 0.5578 Acc: 0.7213
Epoch 46/150
-------------
train Loss: 0.4475 Acc: 0.7825
val Loss: 0.5590 Acc: 0.7189
Epoch 47/150
-------------
train Loss: 0.4436 Acc: 0.7862
val Loss: 0.5591 Acc: 0.7245
Epoch 48/150
-------------
train Loss: 0.4501 Acc: 0.7829
val Loss: 0.5619 Acc: 0.7197
Epoch 49/150
-------------
train Loss: 0.4364 Acc: 0.7868
val Loss: 0.5588 Acc: 0.7229
Epoch 50/150
-------------
train Loss: 0.4394 Acc: 0.7856
val Loss: 0.5621 Acc: 0.7118
Epoch 51/150
-------------
train Loss: 0.4300 Acc: 0.7950
val Loss: 0.5624 Acc: 0.7213
Epoch 52/150
-------------
train Loss: 0.4392 Acc: 0.7854
val Loss: 0.5624 Acc: 0.7165
Epoch 53/150
-------------
train Loss: 0.4332 Acc: 0.7935
val Loss: 0.5619 Acc: 0.7165
Epoch 54/150
-------------
train Loss: 0.4326 Acc: 0.7924
val Loss: 0.5637 Acc: 0.7229
Epoch 55/150
-------------
train Loss: 0.4390 Acc: 0.7929
val Loss: 0.5623 Acc: 0.7181
Epoch 56/150
-------------
train Loss: 0.4327 Acc: 0.7909
val Loss: 0.5623 Acc: 0.7181
Epoch 57/150
-------------
train Loss: 0.4369 Acc: 0.7892
val Loss: 0.5609 Acc: 0.7308
Epoch 58/150
-------------
train Loss: 0.4249 Acc: 0.7993
val Loss: 0.5595 Acc: 0.7276
Epoch 59/150
-------------
train Loss: 0.4264 Acc: 0.7952
val Loss: 0.5645 Acc: 0.7300
Epoch 60/150
-------------
train Loss: 0.4211 Acc: 0.7999
val Loss: 0.5630 Acc: 0.7221
Epoch 61/150
-------------
train Loss: 0.4275 Acc: 0.7952
val Loss: 0.5646 Acc: 0.7237
Epoch 62/150
-------------
train Loss: 0.4124 Acc: 0.8030
val Loss: 0.5640 Acc: 0.7189
Epoch 63/150
-------------
train Loss: 0.4147 Acc: 0.8060
val Loss: 0.5672 Acc: 0.7292
Epoch 64/150
-------------
train Loss: 0.4122 Acc: 0.8052
val Loss: 0.5714 Acc: 0.7221
Epoch 65/150
-------------
train Loss: 0.4069 Acc: 0.8064
val Loss: 0.5735 Acc: 0.7253
Epoch 66/150
-------------
train Loss: 0.4082 Acc: 0.8055
val Loss: 0.5743 Acc: 0.7237
Epoch 67/150
-------------
train Loss: 0.4080 Acc: 0.8057
val Loss: 0.5728 Acc: 0.7276
Epoch 68/150
-------------
train Loss: 0.4071 Acc: 0.8045
val Loss: 0.5738 Acc: 0.7268
Epoch 69/150
-------------
train Loss: 0.4133 Acc: 0.7992
val Loss: 0.5759 Acc: 0.7134
Epoch 70/150
-------------
train Loss: 0.4036 Acc: 0.8118
val Loss: 0.5808 Acc: 0.7229
Epoch 71/150
-------------
train Loss: 0.3994 Acc: 0.8133
val Loss: 0.5796 Acc: 0.7245
Epoch 72/150
-------------
train Loss: 0.4023 Acc: 0.8066
val Loss: 0.5758 Acc: 0.7165
Epoch 73/150
-------------
train Loss: 0.3889 Acc: 0.8150
val Loss: 0.5830 Acc: 0.7237
Epoch 74/150
-------------
train Loss: 0.3954 Acc: 0.8150
val Loss: 0.5790 Acc: 0.7205
Epoch 75/150
-------------
train Loss: 0.3988 Acc: 0.8105
val Loss: 0.5793 Acc: 0.7237
Epoch 76/150
-------------
train Loss: 0.3838 Acc: 0.8221
val Loss: 0.5868 Acc: 0.7411
Epoch 77/150
-------------
train Loss: 0.3861 Acc: 0.8159
val Loss: 0.5818 Acc: 0.7229
Epoch 78/150
-------------
train Loss: 0.3790 Acc: 0.8223
val Loss: 0.5852 Acc: 0.7292
Epoch 79/150
-------------
train Loss: 0.3862 Acc: 0.8182
val Loss: 0.5757 Acc: 0.7197
Epoch 80/150
-------------
train Loss: 0.3821 Acc: 0.8195
val Loss: 0.5770 Acc: 0.7292
Epoch 81/150
-------------
train Loss: 0.3755 Acc: 0.8240
val Loss: 0.5801 Acc: 0.7268
Epoch 82/150
-------------
train Loss: 0.3740 Acc: 0.8226
val Loss: 0.5948 Acc: 0.7260
Epoch 83/150
-------------
train Loss: 0.3713 Acc: 0.8244
val Loss: 0.5905 Acc: 0.7268
Epoch 84/150
-------------
train Loss: 0.3620 Acc: 0.8303
val Loss: 0.5914 Acc: 0.7181
Epoch 85/150
-------------
train Loss: 0.3767 Acc: 0.8238
val Loss: 0.5827 Acc: 0.7221
Epoch 86/150
-------------
train Loss: 0.3738 Acc: 0.8252
val Loss: 0.5903 Acc: 0.7229
Epoch 87/150
-------------
train Loss: 0.3655 Acc: 0.8294
val Loss: 0.6041 Acc: 0.7205
Epoch 88/150
-------------
train Loss: 0.3691 Acc: 0.8280
val Loss: 0.5881 Acc: 0.7197
Epoch 89/150
-------------
train Loss: 0.3684 Acc: 0.8260
val Loss: 0.6188 Acc: 0.7316
Epoch 90/150
-------------
train Loss: 0.3680 Acc: 0.8323
val Loss: 0.5872 Acc: 0.7197
Epoch 91/150
-------------
train Loss: 0.3649 Acc: 0.8281
val Loss: 0.5886 Acc: 0.7340
Epoch 92/150
-------------
train Loss: 0.3587 Acc: 0.8263
val Loss: 0.6003 Acc: 0.7181
Epoch 93/150
-------------
train Loss: 0.3591 Acc: 0.8371
val Loss: 0.5968 Acc: 0.7213
Epoch 94/150
-------------
train Loss: 0.3608 Acc: 0.8266
val Loss: 0.5915 Acc: 0.7165
Epoch 95/150
-------------
train Loss: 0.3581 Acc: 0.8317
val Loss: 0.5907 Acc: 0.7340
Epoch 96/150
-------------
train Loss: 0.3609 Acc: 0.8308
val Loss: 0.5912 Acc: 0.7237
Epoch 97/150
-------------
train Loss: 0.3633 Acc: 0.8273
val Loss: 0.5834 Acc: 0.7276
Epoch 98/150
-------------
train Loss: 0.3539 Acc: 0.8343
val Loss: 0.6014 Acc: 0.7300
Epoch 99/150
-------------
train Loss: 0.3544 Acc: 0.8343
val Loss: 0.5967 Acc: 0.7260
Epoch 100/150
-------------
train Loss: 0.3568 Acc: 0.8302
val Loss: 0.5998 Acc: 0.7213
Epoch 101/150
-------------
train Loss: 0.3543 Acc: 0.8320
val Loss: 0.6053 Acc: 0.7173
Epoch 102/150
-------------
train Loss: 0.3523 Acc: 0.8317
val Loss: 0.5943 Acc: 0.7181
Epoch 103/150
-------------
train Loss: 0.3478 Acc: 0.8360
val Loss: 0.6108 Acc: 0.7268
Epoch 104/150
-------------
train Loss: 0.3462 Acc: 0.8356
val Loss: 0.6079 Acc: 0.7324
Epoch 105/150
-------------
train Loss: 0.3562 Acc: 0.8345
val Loss: 0.6004 Acc: 0.7229
Epoch 106/150
-------------
train Loss: 0.3486 Acc: 0.8355
val Loss: 0.6007 Acc: 0.7395
Epoch 107/150
-------------
train Loss: 0.3487 Acc: 0.8401
val Loss: 0.5940 Acc: 0.7197
Epoch 108/150
-------------
train Loss: 0.3394 Acc: 0.8418
val Loss: 0.6077 Acc: 0.7260
Epoch 109/150
-------------
train Loss: 0.3472 Acc: 0.8354
val Loss: 0.6041 Acc: 0.7165
Epoch 110/150
-------------
train Loss: 0.3445 Acc: 0.8407
val Loss: 0.5990 Acc: 0.7253
Epoch 111/150
-------------
train Loss: 0.3359 Acc: 0.8405
val Loss: 0.6057 Acc: 0.7292
Epoch 112/150
-------------
train Loss: 0.3350 Acc: 0.8446
val Loss: 0.6278 Acc: 0.7268
Epoch 113/150
-------------
train Loss: 0.3396 Acc: 0.8408
val Loss: 0.6067 Acc: 0.7363
Epoch 114/150
-------------
train Loss: 0.3324 Acc: 0.8437
val Loss: 0.6207 Acc: 0.7260
Epoch 115/150
-------------
train Loss: 0.3337 Acc: 0.8444
val Loss: 0.6215 Acc: 0.7292
Epoch 116/150
-------------
train Loss: 0.3420 Acc: 0.8416
val Loss: 0.6173 Acc: 0.7253
Epoch 117/150
-------------
train Loss: 0.3309 Acc: 0.8442
val Loss: 0.6366 Acc: 0.7134
Epoch 118/150
-------------
train Loss: 0.3263 Acc: 0.8448
val Loss: 0.6378 Acc: 0.7197
Epoch 119/150
-------------
train Loss: 0.3274 Acc: 0.8463
val Loss: 0.6276 Acc: 0.7292
Epoch 120/150
-------------
train Loss: 0.3297 Acc: 0.8499
val Loss: 0.6192 Acc: 0.7245
Epoch 121/150
-------------
train Loss: 0.3211 Acc: 0.8498
val Loss: 0.6272 Acc: 0.7165
Epoch 122/150
-------------
train Loss: 0.3267 Acc: 0.8504
val Loss: 0.6198 Acc: 0.7260
Epoch 123/150
-------------
train Loss: 0.3255 Acc: 0.8460
val Loss: 0.6325 Acc: 0.7197
Epoch 124/150
-------------
train Loss: 0.3295 Acc: 0.8482
val Loss: 0.6255 Acc: 0.7205
Epoch 125/150
-------------
train Loss: 0.3279 Acc: 0.8438
val Loss: 0.6390 Acc: 0.7173
Epoch 126/150
-------------
train Loss: 0.3206 Acc: 0.8500
val Loss: 0.6442 Acc: 0.7150
Epoch 127/150
-------------
train Loss: 0.3282 Acc: 0.8452
val Loss: 0.6277 Acc: 0.7213
Epoch 128/150
-------------
train Loss: 0.3238 Acc: 0.8457
val Loss: 0.6318 Acc: 0.7205
Epoch 129/150
-------------
train Loss: 0.3245 Acc: 0.8477
val Loss: 0.6336 Acc: 0.7229
Epoch 130/150
-------------
train Loss: 0.3198 Acc: 0.8513
val Loss: 0.6192 Acc: 0.7245
Epoch 131/150
-------------
train Loss: 0.3229 Acc: 0.8469
val Loss: 0.6246 Acc: 0.7253
Epoch 132/150
-------------
train Loss: 0.3103 Acc: 0.8535
val Loss: 0.6560 Acc: 0.7173
Epoch 133/150
-------------
train Loss: 0.3106 Acc: 0.8559
val Loss: 0.6365 Acc: 0.7245
Epoch 134/150
-------------
train Loss: 0.3147 Acc: 0.8507
val Loss: 0.6501 Acc: 0.7158
Epoch 135/150
-------------
train Loss: 0.3145 Acc: 0.8551
val Loss: 0.6427 Acc: 0.7173
Epoch 136/150
-------------
train Loss: 0.3136 Acc: 0.8531
val Loss: 0.6403 Acc: 0.7221
Epoch 137/150
-------------
train Loss: 0.3093 Acc: 0.8569
val Loss: 0.6399 Acc: 0.7205
Epoch 138/150
-------------
train Loss: 0.3143 Acc: 0.8512
val Loss: 0.6326 Acc: 0.7245
Epoch 139/150
-------------
train Loss: 0.3047 Acc: 0.8563
val Loss: 0.6550 Acc: 0.7245
Epoch 140/150
-------------
train Loss: 0.3073 Acc: 0.8564
val Loss: 0.6492 Acc: 0.7300
Epoch 141/150
-------------
train Loss: 0.3136 Acc: 0.8524
val Loss: 0.6463 Acc: 0.7189
Epoch 142/150
-------------
train Loss: 0.3040 Acc: 0.8595
val Loss: 0.6471 Acc: 0.7229
Epoch 143/150
-------------
train Loss: 0.3073 Acc: 0.8570
val Loss: 0.6535 Acc: 0.7253
Epoch 144/150
-------------
train Loss: 0.3190 Acc: 0.8522
val Loss: 0.6448 Acc: 0.7197
Epoch 145/150
-------------
train Loss: 0.3149 Acc: 0.8539
val Loss: 0.6444 Acc: 0.7253
Epoch 146/150
-------------
train Loss: 0.3027 Acc: 0.8581
val Loss: 0.6500 Acc: 0.7221
Epoch 147/150
-------------
train Loss: 0.3057 Acc: 0.8594
val Loss: 0.6521 Acc: 0.7197
Epoch 148/150
-------------
train Loss: 0.2974 Acc: 0.8594
val Loss: 0.6627 Acc: 0.7150
Epoch 149/150
-------------
train Loss: 0.3060 Acc: 0.8572
val Loss: 0.6473 Acc: 0.7197
Epoch 150/150
-------------
train Loss: 0.3026 Acc: 0.8573
val Loss: 0.6552 Acc: 0.7197
max_val= 0.7410926365795724
data_manage_CASIA/test_CASIA.txt
1261
1261
test Loss: 0.7090 Acc: 0.7002
data_manage_CASIA/test_CASIA.txt
1261
1261
=== SA_mean ===
-image_size (8, 8, 1)
-image_array_size 3
-SUM: 31.304384011775255
-MAX: 1.0
-MIN: 0.0
(256, 256)
CRF Done!
CRF Done!
Tp_D_NND_M_B_sec00077_txt00065_10388 : TP ( 1 , 1 )
